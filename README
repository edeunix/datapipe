Datapipe project / Streaming de dados em tempo real
-------------------

Etapas:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
0) Antes de tudo. Preparando o ambiente:
-- Atenção: esse tutorial foi executado em um Ubuntu 20.04 LTS com 4GB de RAM e 10GB de disco
-- Seja root, ou utilize sudo em todos os comandos.
0.1) sudo su
0.2) apt update
0.3) apt upgarde
0.4) apt install python docker docker-compose
0.5) curl https://bootstrap.pypa.io/get-pip.py --output get-pip.py
0.6) python get-pip.py
0.7) pip install confluent_kafka
0.8) git clone https://github.com/edeunix/datapipe.git

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
1) Crie o arquivo de comando KSQL (commands.sql)
-- Atenção! Você precisa colocar o arquio commands.sql no container ksql_cli! 
-- Especificamente a pasta dentro do container que deve ter o arquivo commands.sql é /srv/docker/ksql/device_count/
	
1.2) Para gerar o arquivo
	# cd ksql/device_count
	# python gen_sql.py

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
2) Para executar todos os containers pela primeira vez use:
	# cd docker
	# docker-compose up -d --force-recreate --build

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
3) Configurando o Grafana
--Atenção: você deve conseguir acessar a porta 3000 do container Grafana. por exemplo: http://localhost:3000 com usuário e senha "grafana"
3.1) Para configurar o Grafana com influxdb é só seguir o tutorial: https://grafana.com/docs/grafana/latest/datasources/influxdb/
3.1.1) Abra o menu lateral clicando no ícone Grafana no cabeçalho superior.
3.1.2) No menu lateral sob o link Painéis, você deve encontrar um link chamado Fontes de dados.
3.1.3) Clique no botão + Adicionar fonte de dados no cabeçalho superior.
3.1.4) Selecione InfluxDB na lista suspensa Tipo.
3.1.5) Selecione InfluxQL ou Flux na lista Query Language.
3.2) Agora você precisa criar um painel: siga o tutorial da atividade para ver como fazer um SQL básico coletando os dados para um "device" em tempo real. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
4) Agora vamos gerar os dados fake pelo produtor. Essa será nossa fonte de dados original. Nossos logs de entrada.
	# cd producer
	# python producer_confluent.py -i data_all.json -t time

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
5) Adicional: se quiser derrubar os dockers é só rodar
	# cd docker
	# docker-compose down
